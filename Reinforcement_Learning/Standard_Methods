                                                    ############## STEPPING ############

gym.Env.step(self, action: ActType) → Union[Tuple[ObsType, float, bool, bool, dict], Tuple[ObsType, float, bool, dict]]
Run one timestep of the environment’s dynamics.

When end of episode is reached, you are responsible for calling reset() to reset this environment’s state. Accepts an action and returns either a tuple (observation, reward,
terminated, truncated, info), or a tuple (observation, reward, done, info). The latter is deprecated and will be removed in future versions.

PARAMETERS:
action (ActType) – an action provided by the agent

RETURNS:
observation (object) – this will be an element of the environment’s observation_space. This may, for instance, be a numpy array containing the positions and velocities of certain objects.

reward (float) – The amount of reward returned as a result of taking the action.

terminated (bool) – whether a terminal state (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results.

truncated (bool) – whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent
physically going out of bounds. Can be used to end the episode prematurely before a terminal state is reached.

info (dictionary) – info contains auxiliary diagnostic information (helpful for debugging, learning, and logging). This might, for instance, contain: metrics that describe
the agent’s performance state, variables that are hidden from observations, or individual reward terms that are combined to produce the total reward.
It also can contain information that distinguishes truncation and termination, however this is deprecated in favour of returning two booleans, and will be removed in a future version.

done (bool) – A boolean value for if the episode has ended, in which case further step() calls will return undefined results. A done signal may be emitted for
different reasons: Maybe the task underlying the environment was solved successfully, a certain timelimit was exceeded, or the physics simulation has entered an invalid state.


                                                            ############ RESETTING ###########
gym.Env.reset(self, *, seed: Optional[int] = None, return_info: bool = False, options: Optional[dict] = None) → Union[ObsType, Tuple[ObsType, dict]]
Resets the environment to an initial state and returns the initial observation.

This method can reset the environment’s random number generator(s) if seed is an integer or if the environment has not yet initialized a random number generator.
If the environment already has a random number generator and reset() is called with seed=None, the RNG should not be reset. Moreover, reset() should (in the typical use case)
be called with an integer seed right after initialization and then never again.

PARAMETERS:
seed (optional int) – The seed that is used to initialize the environment’s PRNG. If the environment does not already have a PRNG and seed=None (the default option) is passed,
a seed will be chosen from some source of entropy (e.g. timestamp or /dev/urandom). However, if the environment already has a PRNG and seed=None is passed, the PRNG will not be reset.
If you pass an integer, the PRNG will be reset even if it already exists. Usually, you want to pass an integer right after the environment has been initialized and then never again.
Please refer to the minimal example above to see this paradigm in action.

return_info (bool) – If true, return additional information along with initial observation. This info should be analogous to the info returned in step()

options (optional dict) – Additional information to specify how the environment is reset (optional, depending on the specific environment)

RETURNS:
observation (object) – Observation of the initial state. This will be an element of observation_space (typically a numpy array) and is analogous to the observation returned by step().

info (optional dictionary) – This will only be returned if return_info=True is passed. It contains auxiliary information complementing observation. This dictionary should be
analogous to the info returned by step().


                                                                ########## RENDERING ###########
gym.Env.render(self: object, *args: Tuple[Any], **kwargs: Dict[str, Any]) → Optional[Union[RenderFrame, List[RenderFrame]]]
It just helps us to visualize the model.